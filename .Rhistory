geog_its_raw$precision.model1 <- pred_detail_df$TP[[3]]/(pred_detail_df$TP[[3]] + pred_detail_df$FP[[3]])
#Recall = TP/(TP + FN)
geog_its_raw$recall.model1 <- pred_detail_df$TP[[3]]/(pred_detail_df$TP[[3]] + pred_detail_df$FN[[3]])
#Finally, calculate Kappa for your model according to:
#First generate the table of comparisons
table1 <- table(geog_its_raw$level.up, geog_its_raw$threshold.pred1)
#Convert to matrix
matrix1 <- as.matrix(table1)
#Calculate kappa
kappa(matrix1, exact = TRUE)/kappa(matrix1)
#Now choose a different threshold value and repeat these diagnostics. What conclusions can you draw about your two thresholds?
#Look at the ROC plot for your first model. Based on this plot choose a probability threshold that balances capturing the most correct predictions against false positives. Then generate a new variable in your data set that classifies each student according to your chosen threshold.
#Create a data frame - I think a table would be useful to make the choice
pred_detail_df <- data.frame(pred.detail@cutoffs,
pred.detail@tp,
pred.detail@fp,
pred.detail@tn,
pred.detail@fn,
perf.detail@y.values,
perf.detail@x.values
)
colnames(pred_detail_df) <- c("Cutoffs", "TP", "FP", "TN", "FN", "TPR", "FPR")
#Print the DF - can compare with plot above
pred_detail_df
#Set treshold at 0.6125 - this gives a TPR of 0.895 and a FPR of 0.22
#On the graph this is right where the second bend is, which I think is a maximising point.
#This would be particularly true if we assume that detecting true positives (sucessfully predicting that a student leveled up, is more important than committing
#false positives (predicting that a student will level up when they didn't).
#This is the case here - a false positive is more punishing than a false negative (student leveled up, but model predicted failure; FNR is 1-TPR)
#A false positive might deprive a student of support to level up, whereas a false negative is a happy surprise
geog_its_raw$threshold.pred1 <- geog_its_raw$pred >= 0.6125
#Now generate three diagnostics
#Accuracy = (TP + TN)/Total
geog_its_raw$accuracy.model1 <- (pred_detail_df$TP[[3]] + pred_detail_df$TN[[3]])/1000
#Precision = TP/(TP + FP)
geog_its_raw$precision.model1 <- pred_detail_df$TP[[3]]/(pred_detail_df$TP[[3]] + pred_detail_df$FP[[3]])
#Recall = TP/(TP + FN)
geog_its_raw$recall.model1 <- pred_detail_df$TP[[3]]/(pred_detail_df$TP[[3]] + pred_detail_df$FN[[3]])
#Finally, calculate Kappa for your model according to:
#First generate the table of comparisons
table1 <- table(geog_its_raw$level.up, geog_its_raw$threshold.pred1)
table1
#Convert to matrix
matrix1 <- as.matrix(table1)
#Calculate kappa
kappa(matrix1, exact = TRUE)/kappa(matrix1)
#Now choose a different threshold value and repeat these diagnostics. What conclusions can you draw about your two thresholds?
writeLines("Accuracy", accuracy.model1)
#Now generate three diagnostics
#Accuracy = (TP + TN)/Total
accuracy.model1 <- (pred_detail_df$TP[[3]] + pred_detail_df$TN[[3]])/1000
writeLines("Accuracy", accuracy.model1)
writeLines(c("Accuracy", accuracy.model1))
#Look at the ROC plot for your first model. Based on this plot choose a probability threshold that balances capturing the most correct predictions against false positives. Then generate a new variable in your data set that classifies each student according to your chosen threshold.
#Create a data frame - I think a table would be useful to make the choice
pred_detail_df <- data.frame(pred.detail@cutoffs,
pred.detail@tp,
pred.detail@fp,
pred.detail@tn,
pred.detail@fn,
perf.detail@y.values,
perf.detail@x.values
)
colnames(pred_detail_df) <- c("Cutoffs", "TP", "FP", "TN", "FN", "TPR", "FPR")
#Print the DF - can compare with plot above
pred_detail_df
#Set treshold at 0.6125 - this gives a TPR of 0.895 and a FPR of 0.22
#On the graph this is right where the second bend is, which I think is a maximising point.
#This would be particularly true if we assume that detecting true positives (sucessfully predicting that a student leveled up, is more important than committing
#false positives (predicting that a student will level up when they didn't).
#This is the case here - a false positive is more punishing than a false negative (student leveled up, but model predicted failure; FNR is 1-TPR)
#A false positive might deprive a student of support to level up, whereas a false negative is a happy surprise
geog_its_raw$threshold.pred1 <- geog_its_raw$pred >= 0.6125
#Now generate three diagnostics
#Accuracy = (TP + TN)/Total
accuracy.model1 <- (pred_detail_df$TP[[3]] + pred_detail_df$TN[[3]])/1000
writeLines(c("Accuracy", accuracy.model1))
#Precision = TP/(TP + FP)
precision.model1 <- pred_detail_df$TP[[3]]/(pred_detail_df$TP[[3]] + pred_detail_df$FP[[3]])
writeLines(c("Precision", precision.model1))
#Recall = TP/(TP + FN)
recall.model1 <- pred_detail_df$TP[[3]]/(pred_detail_df$TP[[3]] + pred_detail_df$FN[[3]])
writeLines(c("Recall", recall.model1))
#Finally, calculate Kappa for your model according to:
#First generate the table of comparisons
table1 <- table(geog_its_raw$level.up, geog_its_raw$threshold.pred1)
table1
#Convert to matrix
matrix1 <- as.matrix(table1)
#Calculate kappa
kappa(matrix1, exact = TRUE)/kappa(matrix1)
#Now choose a different threshold value and repeat these diagnostics. What conclusions can you draw about your two thresholds?
#Look at the ROC plot for your first model. Based on this plot choose a probability threshold that balances capturing the most correct predictions against false positives. Then generate a new variable in your data set that classifies each student according to your chosen threshold.
#Create a data frame - I think a table would be useful to make the choice
pred_detail_df <- data.frame(pred.detail@cutoffs,
pred.detail@tp,
pred.detail@fp,
pred.detail@tn,
pred.detail@fn,
perf.detail@y.values,
perf.detail@x.values
)
colnames(pred_detail_df) <- c("Cutoffs", "TP", "FP", "TN", "FN", "TPR", "FPR")
#Print the DF - can compare with plot above
pred_detail_df
#Set treshold at 0.6125 - this gives a TPR of 0.895 and a FPR of 0.22
#On the graph this is right where the second bend is, which I think is a maximising point.
#This would be particularly true if we assume that detecting true positives (sucessfully predicting that a student leveled up, is more important than committing
#false positives (predicting that a student will level up when they didn't).
#This is the case here - a false positive is more punishing than a false negative (student leveled up, but model predicted failure; FNR is 1-TPR)
#A false positive might deprive a student of support to level up, whereas a false negative is a happy surprise
geog_its_raw$threshold.pred1 <- geog_its_raw$pred >= 0.6125
#Now generate three diagnostics
#Accuracy = (TP + TN)/Total
accuracy.model1 <- (pred_detail_df$TP[[3]] + pred_detail_df$TN[[3]])/1000
writeLines(c("Accuracy", accuracy.model1, ""))
#Precision = TP/(TP + FP)
precision.model1 <- pred_detail_df$TP[[3]]/(pred_detail_df$TP[[3]] + pred_detail_df$FP[[3]])
writeLines(c("Precision", precision.model1, ""))
#Recall = TP/(TP + FN)
recall.model1 <- pred_detail_df$TP[[3]]/(pred_detail_df$TP[[3]] + pred_detail_df$FN[[3]])
writeLines(c("Recall", recall.model1, ""))
#Finally, calculate Kappa for your model according to:
#First generate the table of comparisons
table1 <- table(geog_its_raw$level.up, geog_its_raw$threshold.pred1)
table1
#Convert to matrix
matrix1 <- as.matrix(table1)
#Calculate kappa
kappa(matrix1, exact = TRUE)/kappa(matrix1)
#Now choose a different threshold value and repeat these diagnostics. What conclusions can you draw about your two thresholds?
#Look at the ROC plot for your first model. Based on this plot choose a probability threshold that balances capturing the most correct predictions against false positives. Then generate a new variable in your data set that classifies each student according to your chosen threshold.
#Create a data frame - I think a table would be useful to make the choice
pred_detail_df <- data.frame(pred.detail@cutoffs,
pred.detail@tp,
pred.detail@fp,
pred.detail@tn,
pred.detail@fn,
perf.detail@y.values,
perf.detail@x.values
)
colnames(pred_detail_df) <- c("Cutoffs", "TP", "FP", "TN", "FN", "TPR", "FPR")
#Print the DF - can compare with plot above
pred_detail_df
#Set treshold at 0.6125 - this gives a TPR of 0.895 and a FPR of 0.22
#On the graph this is right where the second bend is, which I think is a maximising point.
#This would be particularly true if we assume that detecting true positives (sucessfully predicting that a student leveled up, is more important than committing
#false positives (predicting that a student will level up when they didn't).
#This is the case here - a false positive is more punishing than a false negative (student leveled up, but model predicted failure; FNR is 1-TPR)
#A false positive might deprive a student of support to level up, whereas a false negative is a happy surprise
geog_its_raw$threshold.pred1 <- geog_its_raw$pred >= 0.6125
#Now generate three diagnostics
#Accuracy = (TP + TN)/Total
accuracy.model1 <- (pred_detail_df$TP[[3]] + pred_detail_df$TN[[3]])/1000
writeLines(c("Accuracy", accuracy.model1, ""))
#Precision = TP/(TP + FP)
precision.model1 <- pred_detail_df$TP[[3]]/(pred_detail_df$TP[[3]] + pred_detail_df$FP[[3]])
writeLines(c("Precision", precision.model1, ""))
#Recall = TP/(TP + FN)
recall.model1 <- pred_detail_df$TP[[3]]/(pred_detail_df$TP[[3]] + pred_detail_df$FN[[3]])
writeLines(c("Recall", recall.model1, ""))
#Finally, calculate Kappa for your model according to:
#First generate the table of comparisons
table1 <- table(geog_its_raw$level.up, geog_its_raw$threshold.pred1)
table1
#Convert to matrix
matrix1 <- as.matrix(table1)
#Calculate kappa
kappa(matrix1, exact = TRUE)#/kappa(matrix1)
#Look at the ROC plot for your first model. Based on this plot choose a probability threshold that balances capturing the most correct predictions against false positives. Then generate a new variable in your data set that classifies each student according to your chosen threshold.
#Create a data frame - I think a table would be useful to make the choice
pred_detail_df <- data.frame(pred.detail@cutoffs,
pred.detail@tp,
pred.detail@fp,
pred.detail@tn,
pred.detail@fn,
perf.detail@y.values,
perf.detail@x.values
)
colnames(pred_detail_df) <- c("Cutoffs", "TP", "FP", "TN", "FN", "TPR", "FPR")
#Print the DF - can compare with plot above
pred_detail_df
#Set treshold at 0.6125 - this gives a TPR of 0.895 and a FPR of 0.22
#On the graph this is right where the second bend is, which I think is a maximising point.
#This would be particularly true if we assume that detecting true positives (sucessfully predicting that a student leveled up, is more important than committing
#false positives (predicting that a student will level up when they didn't).
#This is the case here - a false positive is more punishing than a false negative (student leveled up, but model predicted failure; FNR is 1-TPR)
#A false positive might deprive a student of support to level up, whereas a false negative is a happy surprise
geog_its_raw$threshold.pred1 <- geog_its_raw$pred >= 0.6125
#Now generate three diagnostics
#Accuracy = (TP + TN)/Total
accuracy.model1 <- (pred_detail_df$TP[[3]] + pred_detail_df$TN[[3]])/1000
writeLines(c("Accuracy", accuracy.model1, ""))
#Precision = TP/(TP + FP)
precision.model1 <- pred_detail_df$TP[[3]]/(pred_detail_df$TP[[3]] + pred_detail_df$FP[[3]])
writeLines(c("Precision", precision.model1, ""))
#Recall = TP/(TP + FN)
recall.model1 <- pred_detail_df$TP[[3]]/(pred_detail_df$TP[[3]] + pred_detail_df$FN[[3]])
writeLines(c("Recall", recall.model1, ""))
#Finally, calculate Kappa for your model according to:
#First generate the table of comparisons
table1 <- table(geog_its_raw$level.up, geog_its_raw$threshold.pred1)
table1
#Convert to matrix
matrix1 <- as.matrix(table1)
#Calculate kappa
#kappa(matrix1, exact = TRUE)/
kappa(matrix1)
#Look at the ROC plot for your first model. Based on this plot choose a probability threshold that balances capturing the most correct predictions against false positives. Then generate a new variable in your data set that classifies each student according to your chosen threshold.
#Create a data frame - I think a table would be useful to make the choice
pred_detail_df <- data.frame(pred.detail@cutoffs,
pred.detail@tp,
pred.detail@fp,
pred.detail@tn,
pred.detail@fn,
perf.detail@y.values,
perf.detail@x.values
)
colnames(pred_detail_df) <- c("Cutoffs", "TP", "FP", "TN", "FN", "TPR", "FPR")
#Print the DF - can compare with plot above
pred_detail_df
#Set treshold at 0.6125 - this gives a TPR of 0.895 and a FPR of 0.22
#On the graph this is right where the second bend is, which I think is a maximising point.
#This would be particularly true if we assume that detecting true positives (sucessfully predicting that a student leveled up, is more important than committing
#false positives (predicting that a student will level up when they didn't).
#This is the case here - a false positive is more punishing than a false negative (student leveled up, but model predicted failure; FNR is 1-TPR)
#A false positive might deprive a student of support to level up, whereas a false negative is a happy surprise
geog_its_raw$threshold.pred1 <- geog_its_raw$pred >= 0.6125
#Now generate three diagnostics
#Accuracy = (TP + TN)/Total
accuracy.model1 <- (pred_detail_df$TP[[3]] + pred_detail_df$TN[[3]])/1000
writeLines(c("Accuracy", accuracy.model1, ""))
#Precision = TP/(TP + FP)
precision.model1 <- pred_detail_df$TP[[3]]/(pred_detail_df$TP[[3]] + pred_detail_df$FP[[3]])
writeLines(c("Precision", precision.model1, ""))
#Recall = TP/(TP + FN)
recall.model1 <- pred_detail_df$TP[[3]]/(pred_detail_df$TP[[3]] + pred_detail_df$FN[[3]])
writeLines(c("Recall", recall.model1, ""))
#Finally, calculate Kappa for your model according to:
#First generate the table of comparisons
table1 <- table(geog_its_raw$level.up, geog_its_raw$threshold.pred1)
table1
#Convert to matrix
matrix1 <- as.matrix(table1)
#Calculate kappa
kappa(matrix1, exact = TRUE)/kappa(matrix1)
#Now choose a different threshold value and repeat these diagnostics. What conclusions can you draw about your two thresholds?
geog_its_raw$threshold.pred2 <- geog_its_raw$pred >= 0.84400000
#Now generate three diagnostics
#Accuracy = (TP + TN)/Total
accuracy.model2 <- (pred_detail_df$TP[[2]] + pred_detail_df$TN[[2]])/1000
writeLines(c("Accuracy", accuracy.model2, ""))
#Precision = TP/(TP + FP)
precision.model2 <- pred_detail_df$TP[[2]]/(pred_detail_df$TP[[2]] + pred_detail_df$FP[[2]])
writeLines(c("Precision", precision.model2, ""))
#Recall = TP/(TP + FN)
recall.model2 <- pred_detail_df$TP[[2]]/(pred_detail_df$TP[[2]] + pred_detail_df$FN[[2]])
writeLines(c("Recall", recall.model2, ""))
#Finally, calculate Kappa for your model according to:
#First generate the table of comparisons
table2 <- table(geog_its_raw$level.up, geog_its_raw$threshold.pred2)
table2
#Convert to matrix
matrix2 <- as.matrix(table2)
#Calculate kappa
kappa(matrix2, exact = TRUE)/kappa(matrix2)
#Look at the ROC plot for your first model. Based on this plot choose a probability threshold that balances capturing the most correct predictions against false positives. Then generate a new variable in your data set that classifies each student according to your chosen threshold.
#Create a data frame - I think a table would be useful to make the choice
pred_detail_df <- data.frame(pred.detail@cutoffs,
pred.detail@tp,
pred.detail@fp,
pred.detail@tn,
pred.detail@fn,
perf.detail@y.values,
perf.detail@x.values
)
colnames(pred_detail_df) <- c("Cutoffs", "TP", "FP", "TN", "FN", "TPR", "FPR")
#Print the DF - can compare with plot above
pred_detail_df
#Set treshold at 0.6125 - this gives a TPR of 0.895 and a FPR of 0.22
#On the graph this is right where the second bend is, which I think is a maximising point.
#This would be particularly true if we assume that detecting true positives (sucessfully predicting that a student leveled up, is more important than committing
#false positives (predicting that a student will level up when they didn't).
#This is the case here - a false positive is more punishing than a false negative (student leveled up, but model predicted failure; FNR is 1-TPR)
#A false positive might deprive a student of support to level up, whereas a false negative is a happy surprise
geog_its_raw$threshold.pred1 <- geog_its_raw$pred >= 0.6125
#Now generate three diagnostics
#Accuracy = (TP + TN)/Total
accuracy.model1 <- (pred_detail_df$TP[[3]] + pred_detail_df$TN[[3]])/1000
writeLines(c("Accuracy", accuracy.model1, ""))
#Precision = TP/(TP + FP)
precision.model1 <- pred_detail_df$TP[[3]]/(pred_detail_df$TP[[3]] + pred_detail_df$FP[[3]])
writeLines(c("Precision", precision.model1, ""))
#Recall = TP/(TP + FN)
recall.model1 <- pred_detail_df$TP[[3]]/(pred_detail_df$TP[[3]] + pred_detail_df$FN[[3]])
writeLines(c("Recall", recall.model1, ""))
#Finally, calculate Kappa for your model according to:
#First generate the table of comparisons
table1 <- table(geog_its_raw$level.up, geog_its_raw$threshold.pred1)
table1
#Convert to matrix
matrix1 <- as.matrix(table1)
#Calculate kappa
kappa(matrix1, exact = TRUE)/kappa(matrix1)
#Now choose a different threshold value and repeat these diagnostics. What conclusions can you draw about your two thresholds?
geog_its_raw$threshold.pred2 <- geog_its_raw$pred >= 0.844
#Now generate three diagnostics
#Accuracy = (TP + TN)/Total
accuracy.model2 <- (pred_detail_df$TP[[2]] + pred_detail_df$TN[[2]])/1000
writeLines(c("Accuracy", accuracy.model2, ""))
#Precision = TP/(TP + FP)
precision.model2 <- pred_detail_df$TP[[2]]/(pred_detail_df$TP[[2]] + pred_detail_df$FP[[2]])
writeLines(c("Precision", precision.model2, ""))
#Recall = TP/(TP + FN)
recall.model2 <- pred_detail_df$TP[[2]]/(pred_detail_df$TP[[2]] + pred_detail_df$FN[[2]])
writeLines(c("Recall", recall.model2, ""))
#Finally, calculate Kappa for your model according to:
#First generate the table of comparisons
table2 <- table(geog_its_raw$level.up, geog_its_raw$threshold.pred2)
table2
#Convert to matrix
matrix2 <- as.matrix(table2)
#Calculate kappa
kappa(matrix2, exact = TRUE)/kappa(matrix2)
#Now choose a different threshold value and repeat these diagnostics. What conclusions can you draw about your two thresholds?
geog_its_raw$threshold.pred2 <- geog_its_raw$pred > 0.844
#Now generate three diagnostics
#Accuracy = (TP + TN)/Total
accuracy.model2 <- (pred_detail_df$TP[[2]] + pred_detail_df$TN[[2]])/1000
writeLines(c("Accuracy", accuracy.model2, ""))
#Precision = TP/(TP + FP)
precision.model2 <- pred_detail_df$TP[[2]]/(pred_detail_df$TP[[2]] + pred_detail_df$FP[[2]])
writeLines(c("Precision", precision.model2, ""))
#Recall = TP/(TP + FN)
recall.model2 <- pred_detail_df$TP[[2]]/(pred_detail_df$TP[[2]] + pred_detail_df$FN[[2]])
writeLines(c("Recall", recall.model2, ""))
#Finally, calculate Kappa for your model according to:
#First generate the table of comparisons
table2 <- table(geog_its_raw$level.up, geog_its_raw$threshold.pred2)
table2
#Convert to matrix
matrix2 <- as.matrix(table2)
#Calculate kappa
kappa(matrix2, exact = TRUE)/kappa(matrix2)
#Look at the ROC plot for your first model. Based on this plot choose a probability threshold that balances capturing the most correct predictions against false positives. Then generate a new variable in your data set that classifies each student according to your chosen threshold.
#Create a data frame - I think a table would be useful to make the choice
pred_detail_df <- data.frame(pred.detail@cutoffs,
pred.detail@tp,
pred.detail@fp,
pred.detail@tn,
pred.detail@fn,
perf.detail@y.values,
perf.detail@x.values
)
colnames(pred_detail_df) <- c("Cutoffs", "TP", "FP", "TN", "FN", "TPR", "FPR")
#Print the DF - can compare with plot above
pred_detail_df
#Set treshold at 0.6125 - this gives a TPR of 0.895 and a FPR of 0.22
#On the graph this is right where the second bend is, which I think is a maximising point.
#This would be particularly true if we assume that detecting true positives (sucessfully predicting that a student leveled up, is more important than committing
#false positives (predicting that a student will level up when they didn't).
#This is the case here - a false positive is more punishing than a false negative (student leveled up, but model predicted failure; FNR is 1-TPR)
#A false positive might deprive a student of support to level up, whereas a false negative is a happy surprise
geog_its_raw$threshold.pred1 <- geog_its_raw$pred >= 0.6125
#Now generate three diagnostics
#Accuracy = (TP + TN)/Total
accuracy.model1 <- (pred_detail_df$TP[[3]] + pred_detail_df$TN[[3]])/1000
writeLines(c("Accuracy", accuracy.model1, ""))
#Precision = TP/(TP + FP)
precision.model1 <- pred_detail_df$TP[[3]]/(pred_detail_df$TP[[3]] + pred_detail_df$FP[[3]])
writeLines(c("Precision", precision.model1, ""))
#Recall = TP/(TP + FN)
recall.model1 <- pred_detail_df$TP[[3]]/(pred_detail_df$TP[[3]] + pred_detail_df$FN[[3]])
writeLines(c("Recall", recall.model1, ""))
#Finally, calculate Kappa for your model according to:
#First generate the table of comparisons
table1 <- table(geog_its_raw$level.up, geog_its_raw$threshold.pred1)
table1
#Convert to matrix
matrix1 <- as.matrix(table1)
#Calculate kappa
kappa(matrix1, exact = TRUE)/kappa(matrix1)
#Now choose a different threshold value and repeat these diagnostics. What conclusions can you draw about your two thresholds?
geog_its_raw$threshold.pred2 <- geog_its_raw$pred >= 0.844
#Now generate three diagnostics
#Accuracy = (TP + TN)/Total
accuracy.model2 <- (pred_detail_df$TP[[2]] + pred_detail_df$TN[[2]])/1000
writeLines(c("Accuracy", accuracy.model2, ""))
#Precision = TP/(TP + FP)
precision.model2 <- pred_detail_df$TP[[2]]/(pred_detail_df$TP[[2]] + pred_detail_df$FP[[2]])
writeLines(c("Precision", precision.model2, ""))
#Recall = TP/(TP + FN)
recall.model2 <- pred_detail_df$TP[[2]]/(pred_detail_df$TP[[2]] + pred_detail_df$FN[[2]])
writeLines(c("Recall", recall.model2, ""))
#Finally, calculate Kappa for your model according to:
#First generate the table of comparisons
table2 <- table(geog_its_raw$level.up, geog_its_raw$threshold.pred2)
table2
#Convert to matrix
matrix2 <- as.matrix(table2)
#Calculate kappa
kappa(matrix2, exact = TRUE)/kappa(matrix2)
geog_its_raw$pred[[100]]
geog_its_raw$threshold.pred1[[100]]
#Look at the ROC plot for your first model. Based on this plot choose a probability threshold that balances capturing the most correct predictions against false positives. Then generate a new variable in your data set that classifies each student according to your chosen threshold.
#Create a data frame - I think a table would be useful to make the choice
pred_detail_df <- data.frame(pred.detail@cutoffs,
pred.detail@tp,
pred.detail@fp,
pred.detail@tn,
pred.detail@fn,
perf.detail@y.values,
perf.detail@x.values
)
colnames(pred_detail_df) <- c("Cutoffs", "TP", "FP", "TN", "FN", "TPR", "FPR")
#Print the DF - can compare with plot above
pred_detail_df
#Set treshold at 0.6125 - this gives a TPR of 0.895 and a FPR of 0.22
#On the graph this is right where the second bend is, which I think is a maximising point.
#This would be particularly true if we assume that detecting true positives (sucessfully predicting that a student leveled up, is more important than committing
#false positives (predicting that a student will level up when they didn't).
#This is the case here - a false positive is more punishing than a false negative (student leveled up, but model predicted failure; FNR is 1-TPR)
#A false positive might deprive a student of support to level up, whereas a false negative is a happy surprise
geog_its_raw$threshold.pred1 <- geog_its_raw$pred >= 0.6124
#Now generate three diagnostics
#Accuracy = (TP + TN)/Total
accuracy.model1 <- (pred_detail_df$TP[[3]] + pred_detail_df$TN[[3]])/1000
writeLines(c("Accuracy", accuracy.model1, ""))
#Precision = TP/(TP + FP)
precision.model1 <- pred_detail_df$TP[[3]]/(pred_detail_df$TP[[3]] + pred_detail_df$FP[[3]])
writeLines(c("Precision", precision.model1, ""))
#Recall = TP/(TP + FN)
recall.model1 <- pred_detail_df$TP[[3]]/(pred_detail_df$TP[[3]] + pred_detail_df$FN[[3]])
writeLines(c("Recall", recall.model1, ""))
#Finally, calculate Kappa for your model according to:
#First generate the table of comparisons
table1 <- table(geog_its_raw$level.up, geog_its_raw$threshold.pred1)
table1
#Convert to matrix
matrix1 <- as.matrix(table1)
#Calculate kappa
kappa(matrix1, exact = TRUE)/kappa(matrix1)
#Now choose a different threshold value and repeat these diagnostics. What conclusions can you draw about your two thresholds?
geog_its_raw$threshold.pred2 <- geog_its_raw$pred >= 0.844
#Now generate three diagnostics
#Accuracy = (TP + TN)/Total
accuracy.model2 <- (pred_detail_df$TP[[2]] + pred_detail_df$TN[[2]])/1000
writeLines(c("Accuracy", accuracy.model2, ""))
#Precision = TP/(TP + FP)
precision.model2 <- pred_detail_df$TP[[2]]/(pred_detail_df$TP[[2]] + pred_detail_df$FP[[2]])
writeLines(c("Precision", precision.model2, ""))
#Recall = TP/(TP + FN)
recall.model2 <- pred_detail_df$TP[[2]]/(pred_detail_df$TP[[2]] + pred_detail_df$FN[[2]])
writeLines(c("Recall", recall.model2, ""))
#Finally, calculate Kappa for your model according to:
#First generate the table of comparisons
table2 <- table(geog_its_raw$level.up, geog_its_raw$threshold.pred2)
table2
#Convert to matrix
matrix2 <- as.matrix(table2)
#Calculate kappa
kappa(matrix2, exact = TRUE)/kappa(matrix2)
#Now choose a different threshold value and repeat these diagnostics. What conclusions can you draw about your two thresholds?
geog_its_raw$threshold.pred2 <- geog_its_raw$pred >= 0.8439
#Now generate three diagnostics
#Accuracy = (TP + TN)/Total
accuracy.model2 <- (pred_detail_df$TP[[2]] + pred_detail_df$TN[[2]])/1000
writeLines(c("Accuracy", accuracy.model2, ""))
#Precision = TP/(TP + FP)
precision.model2 <- pred_detail_df$TP[[2]]/(pred_detail_df$TP[[2]] + pred_detail_df$FP[[2]])
writeLines(c("Precision", precision.model2, ""))
#Recall = TP/(TP + FN)
recall.model2 <- pred_detail_df$TP[[2]]/(pred_detail_df$TP[[2]] + pred_detail_df$FN[[2]])
writeLines(c("Recall", recall.model2, ""))
#Finally, calculate Kappa for your model according to:
#First generate the table of comparisons
table2 <- table(geog_its_raw$level.up, geog_its_raw$threshold.pred2)
table2
#Convert to matrix
matrix2 <- as.matrix(table2)
#Calculate kappa
kappa(matrix2, exact = TRUE)/kappa(matrix2)
0.6125 >= 0.6125
geog_its_raw$pred[100]
geog_its_raw$pred[100] >= 0.6125
(561 + 211)/1000
(561 + 39)
/1000
561 + 189
0.6 * 0.75
189+211
39+211
0.4*0.25
0.772-0.55
1-0.55
0.222/0.45
561+39+189+211
#How did I arrive at a kappa of more than 1?
#Time to do manual calculations
#First model
p01 <- (468 + 358)/1000
pe1 <- (((468 + 132)/1000) * ((468 + 42)/1000)) + (((42 + 358)/1000) * ((132 + 358)/1000))
manual_kappa1 <- (p01 - pe1)/(1 - pe1)
writeLines(c("Cohen's Kappa for the first model = ", manual_kappa1))
#2nd model
p02 <- (561 + 211)/1000
pe2 <- (((561 + 39)/1000) * ((561 + 189)/1000)) + (((189 + 211)/1000) * ((39 + 211)/1000))
manual_kappa2 <- (p02 - pe2)/(1 - pe2)
writeLines(c("Cohen's Kappa for the second model = ", manual_kappa2))
kappa
kappa()
kappa
