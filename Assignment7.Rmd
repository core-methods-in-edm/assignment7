---
title: "Assignment 7"
author: "Charles Lang"
date: "12/1/2016"
output: html_document
---
#Create Data
```{r}
id <- seq(1,1000,1)
D1 <- data.frame(id)
post.test1 <- rnorm(500, 0.4,0.07)
post.test1 <- ifelse(post.test1 < 0, 0, post.test1)
post.test1 <- ifelse(post.test1 > 1, 1, post.test1)
post.test2 <- rnorm(500, 0.6,0.07)
post.test2 <- ifelse(post.test2 < 0, 0, post.test2)
post.test2 <- ifelse(post.test2 > 1, 1, post.test2)
#post.test3 <- rnorm(300, 0.75,0.07)
#post.test3 <- ifelse(post.test3 > 1, 1, post.test3)
D1$post.test.score <- round(c(post.test1,post.test2),2)
D1$pre.test.score <- round(c(post.test1,post.test2), 2)
D1$messages <- round(sample(c(150:200),1000, replace = TRUE)*D1$post.test.score)
D1$forum.posts <- round(sample(c(0:40),1000, replace = TRUE)*(D1$post.test.score) + 2)
D1$av.assignment.score <- round(D1$post.test.score*rnorm(1000, 0.5,0.1),2)
D1$level.up <- ifelse(D1$post.test.score > 0.45 & D1$av.assignment.score > 0.25, "yes", "no")
```

#Visualize the distributions
```{r}
library(ggplot2)
library(dplyr)
library(tidyr)
#HINT: look up "facet" in the ggplot documentation
D2 <- select(D1, 1:7)
#Convert yes/no to 1/0 to avoid mixing variable types
D2$level.up <- ifelse(D2$level.up == "yes", 1,0)
D3 <- gather(D2, "measure", "score", 2:7)
p1 <- ggplot(D3, aes(score)) + facet_wrap(~measure, scales = "free")
p1 + geom_histogram(stat = "count")
#Visualize the relationships between variables
pairs(D2)
```

#Classification tree
```{r}
#Create a classification tree that predicts whether a student "levels up" in the online course using three variables of your choice (remember to set all controls to their minimums)
library(rpart)
c.tree1 <- rpart(level.up ~ forum.posts + pre.test.score, method = "class", data = D1, control=rpart.control(minsplit=1, minbucket=1, cp=0.001))
printcp(c.tree1)
plot(c.tree1)
text(c.tree1)
#Generate a probability value that represents the probability that a student levels up based your classification tree 
D1$pred <- predict(c.tree1, type = "prob")[,2]#Last class we used type = "class" which predicted the classification for us, this time we are using type = "prob" to see the probability that our previous classififcation was based on.
#Now you can generate the ROC curve for your model. You will need to install the package ROCR to do this.
library(ROCR)
#Plot the curve
pred.detail <- prediction(D1$pred, D1$level.up) 
plot(performance(pred.detail, "tpr", "fpr"))
abline(0, 1, lty = 2)
#Calculate the Area Under the Curve
unlist(slot(performance(pred.detail,"auc"), "y.values"))#unlist liberates AUC value from the "performance" object created by ROCR
#Now repeat this process, but using the variables you did not use for the previous model and compare the plots & results of your two models. Which one do you think was the better model? 
```
#Thresholds
```{r}
#Look at the ROC plot for your first model. Based on this plot choose a probability threshold that balances capturing the most correct predictions against false positives. Then generate a new variable in your data set that classifies each student according to your chosen threshold.
D1$threshold.pred1 <- ifelse(D1$pred >= 0.8, "yes", "no")
D1$threshold.pred2 <- ifelse(D1$pred >= 0.95, "yes", "no")
D1$threshold.pred3 <- ifelse(D1$pred >= 0.25, "yes", "no")
#Now generate three diagnostics:
accuracy.model1 <- mean(ifelse(D1$level.up == D1$threshold.pred1, 1, 0))
D1$truepos.model1 <- ifelse(D1$level.up == "yes" & D1$threshold.pred1 == "yes", 1, 0)
D1$falsepos.model1 <- ifelse(D1$level.up == "no" & D1$threshold.pred1 == "yes", 1,0)
D1$falseneg.model1 <- ifelse(D1$level.up == "yes" & D1$threshold.pred1 == "no", 1,0)
precision.model1 <- sum(D1$truepos.model1)/(sum(D1$truepos.model1) + sum(D1$falsepos.model1))
recall.model1 <- sum(D1$truepos.model1)/(sum(D1$truepos.model1) + sum(D1$falseneg.model1))
#Finally, calculate Kappa for your model according to:
#First generate the table of comparisons
table1 <- table(D1$level.up, D1$threshold.pred1)
table1
#Calculate kappa manually
po <- (586+247)/(586+247+14+153)
pe <- ((586 + 14)/(586+247+14+153))*((586 + 153)/(586+247+14+153)) + ((14 + 247)/(586+247+14+153))*((153+247)/(586+247+14+153))
kappa <- (po - pe)/(1 - pe)
#Calculate OOB
library(psych) #You could also use the "irr" or "vcd" library versions
cohen.kappa(table1)
#Now choose a different threshold value and repeat these diagnostics. What conclusions can you draw about your two thresholds?
#Alternate kappa value
library(irr)
kappa2(D1[,c(7,9)], "unweighted")
kappa2(D1[,c(7,10)], "unweighted")
kappa2(D1[,c(7,11)], "unweighted")
```