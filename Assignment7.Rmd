---
title: "EDM Assignment 7"
author: Joellyn Heng
date: 3 December 2019
output: html_document
---

In the following assignment you will be looking at data from an one level of an online geography tutoring system used by 5th grade students. The game involves a pre-test of geography knowledge (pre.test), a series of assignments for which you have the average score (av.assignment.score),  the number of messages sent by each student to other students about the assignments (messages), the number of forum posts students posted asking questions about the assignment (forum.posts), a post test at the end of the level (post.test) and whether or not the system allowed the students to go on to the next level (level.up).  

```{r setup, include= FALSE}
library(rpart)
library(party)
library(rpart.plot)
library(dplyr)
library(plyr)
library(stringr)
library(ggplot2)
library(corrplot)
library(ROCR)
library(caret)
```

## Part I

####Upload data
```{r}
data <- read.csv("online.data.csv")
data <- data %>%
  select(-id)
```

####Visualization 
```{r}
#Start by creating histograms of the distributions for all variables (#HINT: look up "facet" in the ggplot documentation)

histogram1 <- ggplot(data=data, aes(x=post.test.score)) + geom_histogram()
histogram1 + facet_wrap(~level.up)

histogram2 <- ggplot(data=data, aes(x=pre.test.score)) + geom_histogram()
histogram2 + facet_wrap(~level.up)

histogram3 <- ggplot(data=data, aes(x=messages)) + geom_histogram()
histogram3 + facet_wrap(~level.up)

histogram4 <- ggplot(data=data, aes(x=forum.posts)) + geom_histogram()
histogram4 + facet_wrap(~level.up)

histogram5 <- ggplot(data=data, aes(x=av.assignment.score)) + geom_histogram()
histogram5 + facet_wrap(~level.up)

```

####Then visualize the relationships between variables

```{r corr between vars}

data$actual <- ifelse(data$level.up == "yes",1,0)

data <- select(data, -level.up)

COR <- cor(data)

corrplot(COR, order="AOE", method="circle", tl.pos="lt", type="upper",        
tl.col="black", tl.cex=0.6, tl.srt=45, 
        addCoef.col="black", addCoefasPercent = TRUE,
        sig.level=0.50, insig = "blank")

```


####Try to capture an intution about the data and the relationships

The correlogram above shows the correlations between variables. Of interest would be the correlation between "actual", which denotes whether or not the system allowed the student to go on to the next level ("0" being "no", "1" being "yes"), and all the other variables. 

"av.assignment.score" and "post.test.score" have the highest correlation with "actual" among the variables, with correlations of 0.80 and 0.71 respectively. 

#Classification tree

####Create a classification tree that predicts whether a student "levels up" in the online course using three variables of your choice (As we did last time, set all controls to their minimums)

```{r}

rpart1 <- rpart(actual ~ av.assignment.score + messages + post.test.score,  method="class", data=data)

```

####Plot and generate a CP table for your tree 

```{r cp for rpart1}
printcp(rpart1)
rpart.plot(rpart1)

```

####Generate a probability value that represents the probability that a student levels up based your classification tree 

```{r prob for rpart1}

data$pred1 <- predict(rpart1, type = "prob")[,2]
#Last class we used type = "class" which predicted the classification for us, this time we are using type = "prob" to see the probability that our classififcation is based on.

```

## Part II
####Now you can generate the ROC curve for your model. You will need to install the package ROCR to do this.

```{r ROC for rpart1}

#Plot the curve
pred.detail <- prediction(data$pred1, data$actual) 
plot(performance(pred.detail, "tpr", "fpr"))
abline(0, 1, lty = 2)

#Calculate the Area Under the Curve
unlist(slot(performance(pred.detail,"auc"), "y.values"))

#Unlist liberates the AUC value from the "performance" object created by ROCR
```

####Now repeat this process, but using the variables you did not use for the previous model and compare the plots & results of your two models. Which one do you think was the better model? Why?


```{r repeat for rpart2}

rpart2 <- rpart(actual ~ forum.posts + messages + pre.test.score,  method="class", data=data)

printcp(rpart2)
rpart.plot(rpart2)

data$pred2 <- predict(rpart2, type = "prob")[,2]

pred.detail2 <- prediction(data$pred2, data$actual) 
#creates a "prediction" object for classifier evaluation using ROCR

plot(performance(pred.detail2, "tpr", "fpr"))
abline(0, 1, lty = 2)

#Calculate the Area Under the Curve
unlist(slot(performance(pred.detail2,"auc"), "y.values"))

```

First model is a better model as the AUC is larger, in fact at the maximum of 1. Hence rpart1 as a classifier model can fully correctly predict the actual outcome of whether the system lets the student pass or not. This means that the two distribution curves do not overlap, and that the model has an ideal measure of separability.

Model rpart2 on the other hand, has an ROC that indicates an overlap between the two distribution curves, with an AUC of 0.88. This means there is 88% chance that the model will be able to distinguish between positive and negative classes.

## Part III
####Thresholds

####Look at the ROC plot for your first model. Based on this plot choose a probability threshold that balances capturing the most correct predictions against false positives. Then generate a new variable in your data set that classifies each student according to your chosen threshold.

Note: I'll be looking at my second model rpart2, since my first model rpart1 perfectly classifies the predictions.

```{r code from website, include = FALSE}

#perf <- performance(pred.detail2, "tpr", "fpr")
#str(perf)
#S4 class, so we can use @ to access the slots.

#Note to self: This is the cost measure in the ROCR package that you can use to create a performance object. If you use it to find the minimum cost, then it will give you the same cutoff as opt.cut, but not give you the sensitivity and specificity. https://www.r-bloggers.com/a-small-introduction-to-the-rocr-package/

#cost.perf <- performance(pred.detail2, "cost")
#pred.detail2@cutoffs[[1]][which.min(cost.perf@y.values[[1]])]

```

From the ROC graph for my rpart2 model, it seems like FPR between 0.25 to 0.3 balances capturing the most correct predictions against false positives. From trial and error, 0.4 probability threshold generates FPR of 0.267 and TPR of 0.9425. Hence, 0.4 would be the first threshold probability I would choose.

```{r find threshold1}

threshold1.pred2 <- 0.4
data$threshold1 <- ifelse(data$pred2 >= threshold1.pred2, 1, 0)
FPR1 <- sum(data$threshold1 == 1 & data$actual == 0) / sum(data$actual == 0)
TPR1 <- sum(data$threshold1 == 1 & data$actual == 1) / sum(data$actual == 1)

threshold1.pred2
FPR1
TPR1

```


####Now generate three diagnostics:

```{r three diagnostics based on threshold1}

#Accuracy = correct predictions/ total predictions
accuracy1 <- sum(data$actual == data$threshold1) / nrow(data)

#Precision = TP / TP+FP
precision1 <- sum(data$threshold1 == 1 & data$threshold1 == data$actual) / sum(data$threshold1 == 1)

#Recall = TP / TP+FN
recall1 <- sum(data$threshold1 == 1 & data$threshold1 == data$actual) / sum(data$actual == 1)

accuracy1 
precision1
recall1

```

####Finally, calculate Kappa for your model according to:

Note: I decided to use a different function, as the function kappa in R base is not calculating Cohen's Kappa but "Compute or Estimate the Condition Number of a Matrix". When I use the given kappa function, I get a number close to 1 for this threshold, and a number larger than 1 for the second threshold.

```{r kappa based on threshold1}

#First generate the table of comparisons
table1 <- table(data$actual, data$threshold1)

#Convert to matrix
matrix1 <- as.matrix(table1)
matrix1

#Calculate kappa
#kappa(matrix1, exact = TRUE)/kappa(matrix1) #used another function below.

caret::confusionMatrix(matrix1)

```

Kappa is 0.6393.

####Now choose a different threshold value and repeat these diagnostics. What conclusions can you draw about your two thresholds?

I am choosing a different threshold value of 0.65. It gives a FPR of 0.065 and TPR of 0.5275. While it results in a lower FPR than the first threshold, it also results in a much lower TPR.

```{r find threshold2}

threshold2.pred2 <- 0.65
data$threshold2 <- ifelse(data$pred2 >= threshold2.pred2, 1, 0)
FPR2 <- sum(data$threshold2 == 1 & data$actual == 0) / sum(data$actual == 0)
TPR2 <- sum(data$threshold2 == 1 & data$actual == 1) / sum(data$actual == 1)

threshold2.pred2
FPR2
TPR2

```

```{r repeat diagnostics for threshold2}

#Accuracy = correct predictions/ total predictions
accuracy2 <- sum(data$actual == data$threshold2) / nrow(data)

#Precision = TP / TP+FP
precision2 <- sum(data$threshold2 == 1 & data$threshold2 == data$actual) / sum(data$threshold2 == 1)

#Recall = TP / TP+FN
recall2 <- sum(data$threshold2 == 1 & data$threshold2 == data$actual) / sum(data$actual == 1)

accuracy2
precision2
recall2

```

```{r comparing diagnostics}

diags <- as.data.frame(matrix(c(accuracy1, precision1, recall1,accuracy2, precision2, recall2),ncol=3, byrow = TRUE))

colnames(diags) <- c("accuracy", "precision", "recall")
rownames(diags) <- c("threshold1", "threshold2")

diags

```

Recall: `threshold1` is chosen for a high TPR balancing the FPR. With `threshold1` at 0.4, FPR is at 0.267 while TPR is at 0.9425. `threshold2` at 0.65, on the other hand, has FPR lower at 0.065 but a much lower TPR at 0.5275 (therefore further left on the ROC curve). 

`threshold1` has higher accuracy and higher recall. However, it has lower precision, as precision would naturally tend towards 100% the higher the threshold goes. This is because as the threshold goes leftward on the two distrubtion curves), the proportion of TP out of TP + FP tends towards 100%. While precision is larger for `threshold2`, recall (i.e. TP / (TP+FN)) is much smaller than that of `threshold1`, as the threshold level wrongly classifies a large number of positive observations as negatives (i.e. FN).

Hence, `threshold1` seem to be a more optimal threshol level than `threshold2` based on the ROC and the diagnostics, as it balances both TPR and FPR better.

```{r kappa based on threshold2}

#First generate the table of comparisons
table2 <- table(data$actual, data$threshold2)

#Convert to matrix
matrix2 <- as.matrix(table2)

#Calculate kappa
#kappa(matrix2, exact = TRUE)/kappa(matrix2) #used a different function. When I use the kappa() function, my kappa value is larger than 1, which doesn't make sense. 

caret::confusionMatrix(matrix2)

```

Kappa is 0.4933. Compared to `threshold1`, which has a kappa value of 0.6393, `threshold2` has lower reliability between data and model/threshold.

```{r notes to self, include= FALSE}


#Distributions of negatives and positives based on classifier model:
#x-axis: predicted probabilities (0 - negative to 1 - positive), y-axis: count of observations.
#E.g. 10 datapoints have predicted probability of 0.1. Based on observations, 10 of them are negative. 50 datapoints have predicted probability of 0.3. Based on observations, 50 of them are negative. 20 datapoints (10 from each group) that have predicted probability of 0.5, based on observations half is negative, half is positive.

#ROC = Visualizes all possible thresholds from 0 to 1, by plotting the points (FPR, TPR) for each threshold layers on classifier/distributions. 

#If classifier is good at separating the classes (i.e. distributions do not overlap much), ROC curves hugs the top left of the plot. Even if you set threshold at 0.6, TPR can be 0.8 and FPR can still be 0.

#Diagonal line means you have a classifier that is no better than random guessing.

#AUC = a way to quantify how good the classifier is. % of AUC as compared to the whole box.

#You can only look at FPR and TPR after setting a threshold:
#TPR = true positives (right of threshold for right distribution) / all positives (right distribution)
#FPR = false positives (right of threshold for left distribution) / all negatives (left distribution)

#Note: 1) Most real-world problems dont have balanced classes. But it does not change how ROC works. 2) Predicted probabilities look normal for demonstration, but in reality it would not follow.

#Choosing classification threshold is a business decision - whether you want to minimize FPR or maximize TPR.

```
